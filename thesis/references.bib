@inproceedings{Poon2011,
  abstract  = {The key limiting factor in graphical model inference and learning is the complexity of the partition function. We thus ask the question: what are general conditions under which the partition function is tractable? The answer leads to a new kind of deep architecture, which we call sum-product networks (SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and products as internal nodes, and weighted edges. We show that if an SPN is complete and consistent it represents the partition function and all marginals of some graphical model, and give semantics to its nodes. Essentially all tractable graphical models can be cast as SPNs, but SPNs are also strictly more general. We then propose learning algorithms for SPNs, based on backpropagation and EM. Experiments show that inference and learning with SPNs can be both faster and more accurate than with standard deep networks. For example, SPNs perform image completion better than state-of-the-art deep networks for this task. SPNs also have intriguing potential connections to the architecture of the cortex.},
  author    = {Hoifung Poon and Pedro Domingos},
  booktitle = {2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
  month     = {11},
  pages     = {689-690},
  publisher = {IEEE},
  title     = {Sum-product networks: A new deep architecture},
  year      = {2011}
}
@inproceedings{Zhao2015,
  abstract  = {In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of \{\em normal\} SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.},
  author    = {Han Zhao and Mazen Melibari and Pascal Poupart},
  city      = {Lille, France},
  booktitle = {ICML'15 Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  month     = {1},
  pages     = {116-124},
  publisher = {JMLR.org},
  title     = {On the Relationship between Sum-Product Networks and Bayesian Networks},
  year      = {2015}
}
@inproceedings{Bahar1993,
  abstract  = {In this paper we present theory and experiments on the algebraic\ndecision diagrams (ADDs). These diagrams extend BDD's by allowing values\nfrom an arbitrary finite domain to be associated with the terminal\nnodes. We present a treatment founded in Boolean algebras and discuss\nalgorithms and results in applications like matrix multiplication and\nshortest path algorithms. Furthermore, we outline possible applications\nof ADD's to logic synthesis, formal verification, and testing of digital\nsystems},
  author    = {R.I. Bahar and E.A. Frohm and C.M. Gaona and G.D. Hachtel and E. Macii and A. Pardo and F. Somenzi},
  doi       = {10.1109/ICCAD.1993.580054},
  isbn      = {0-8186-4490-7},
  issn      = {09259856},
  booktitle = {Proceedings of 1993 International Conference on Computer Aided Design (ICCAD)},
  pages     = {188-191},
  publisher = {IEEE Comput. Soc. Press},
  title     = {Algebraic decision diagrams and their applications},
  url       = {http://ieeexplore.ieee.org/document/580054/},
  year      = {1993}
}
@book{Koller2009,
  abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality.Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty.The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.Adaptive Computation and Machine Learning series},
  author   = {Daphne Koller and Nir Friedman},
  journal  = {Foundations},
  title    = {Probabilistic Graphical Models: Principles and Techniques},
  year     = {2009}
}
@article{Nie2014,
  abstract = {This work presents novel algorithms for learning Bayesian network structures with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that $k$-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate.},
  author   = {Siqi Nie and Denis Deratani Maua and Cassio Polpo de Campos and Qiang Ji and Denis Deratani Mauá and Cassio Polpo de Campos and Qiang Ji},
  issn     = {10495258},
  journal  = {Advances in Neural Information Processing Systems 27},
  title    = {Advances in Learning Bayesian Networks of Bounded Treewidth},
  year     = {2014}
}
@inproceedings{Peharz2014,
  abstract  = {Sum-product networks (SPNs) are a recently proposed type of probabilistic graphical models allowing complex variable interactions while still granting efficient inference. In this paper we demonstrate the suitability of SPNs for modeling log-spectra of speech signals using the application of artificial bandwidth extension, i.e. artificially replacing the high-frequency content which is lost in telephone signals. We use SPNs as observation models in hidden Markov models (HMMs), which model the temporal evolution of log short-time spectra. Missing frequency bins are replaced by the SPNs using most-probable-explanation inference, where the state-dependent reconstructions are weighted with the HMM state posterior. According to subjective listening and objective evaluation, our system consistently and significantly improves the state of the art. © 2014 IEEE.},
  author    = {Robert Peharz and Georg Kapeller and Pejman Mowlaee and Franz Pernkopf},
  booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  keywords  = {HMM,SPN,graphical models,speech bandwidth extension},
  month     = {5},
  pages     = {3699-3703},
  publisher = {IEEE},
  title     = {Modeling speech with sum-product networks: Application to bandwidth extension},
  year      = {2014}
}
@article{Hsu2017,
  abstract = {Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice. This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique.},
  author   = {Wilson Hsu and Agastya Kalra and Pascal Poupart},
  journal  = {Iclr},
  pages    = {1-10},
  title    = {Online Structure Learning for Sum-product Networks with Gaussian Leaves},
  year     = {2017}
}
@misc{Poupart2017,
  author = {Pascal Poupart},
  title  = {Guest Lecture in STAT946 - Deep Learning (University of Waterloo) on October 17th},
  url    = {https://www.youtube.com/watch?v=Nm0jNqOnQ2o},
  year   = {2017}
}
@inproceedings{Rashwan2016,
  abstract  = {Probabilistic graphical models provide a gen-eral and flexible framework for reasoning about complex dependencies in noisy do-mains with many variables. Among the var-ious types of probabilistic graphical mod-els, sum-product networks (SPNs) have re-cently generated some interest because ex-act inference can always be done in linear time with respect to the size of the network. This is particularly attractive since it means that learning an SPN from data always yields a tractable model for inference. However, existing parameter learning algorithms for SPNs operate in batch mode and do not scale easily to large datasets. In this work, we explore online algorithms to ensure that pa-rameter learning can also be done tractably with respect to the amount of data. More specifically, we propose a new Bayesian mo-ment matching (BMM) algorithm that oper-ates naturally in an online fashion and that can be easily distributed. We demonstrate the effectiveness and scalability of BMM in comparison to online extensions of gradient descent, exponentiated gradient and expecta-tion maximization on 20 classic benchmarks and 4 large scale datasets.},
  author    = {Abdullah Rashwan and Han Zhao and Pascal Poupart},
  booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016},
  pages     = {1469-1477},
  title     = {Online and distributed Bayesian moment matching for parameter learning in sum-product networks},
  year      = {2016}
}
@inbook{Lee2013,
  abstract = {Sum–product networks (SPNs) are deep architectures that can learn and infer at low computational costs. The structure of SPNs is especially impor-tant for their performance; however, structure learning for SPNs has until now been introduced only for batch-type dataset. In this study, we propose a new on-line incremental structure learning method for SPNs. We note that SPNs can be represented by mixtures of basis distributions. Online learning of SPNs can be formulated as an online clustering problem, in which a local assigning instance corresponds to modifying the tree-structure of the SPN incrementally. In the method, the number of hidden units and even layers are evolved dynamically on incoming data. The experimental results show that the proposed method outper-forms the online version of the previous method. In addition, it achieves the performance of batch structure learning.},
  author   = {Sang-Woo Lee and Min-Oh Heo and Byoung-Tak Zhang},
  journal  = {LNCS},
  pages    = {220-227},
  title    = {Online Incremental Structure Learning of Sum–Product Networks},
  volume   = {8227},
  year     = {2013}
}
@article{Peharz2015,
  author      = {Robert Peharz},
  institution = {Graz University of Technology},
  title       = {Foundations of Sum-Product Networks for Probabilistic Modeling},
  year        = {2015}
}
@article{Amer2016,
  abstract = {© 1979-2012 IEEE. This paper addresses detection and localization of human activities in videos. We focus on activities that may have variable spatiotemporal arrangements of parts, and numbers of actors. Such activities are represented by a sum-product network (SPN). A product node in SPN represents a particular arrangement of parts, and a sum node represents alternative arrangements. The sums and products are hierarchically organized, and grounded onto space-time windows covering the video. The windows provide evidence about the activity classes based on the Counting Grid (CG) model of visual words. This evidence is propagated bottom-up and top-down to parse the SPN graph for the explanation of the video. The node connectivity and model parameters of SPN and CG are jointly learned under two settings, weakly supervised, and supervised. For evaluation, we use our new Volleyball dataset, along with the benchmark datasets VIRAT, UT-Interactions, KTH, and TRECVID MED 2011. Our video classification and activity localization are superior to those of the state of the art on these datasets.},
  author   = {Mohamed R. Amer and Sinisa Todorovic},
  issue    = {4},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Activity Recognition,Hierarchical Models,Sum-Product Networks},
  month    = {4},
  pages    = {800-813},
  title    = {Sum Product Networks for Activity Recognition},
  volume   = {38},
  year     = {2016}
}
@inproceedings{Cheng2014,
  abstract  = {Sum product networks (SPNs) are a new class of deep probabilistic models. They can contain multiple hidden layers while keeping their inference and training times tractable. An SPN consists of interleaving layers of sum nodes and product nodes. A sum node can be interpreted as a hidden variable, and a product node can be viewed as a feature capturing rich interactions among an SPN’s inputs. We show that the ability of SPN to use hidden layers to model complex dependencies among words, and its tractable inference and learning times, make it a suitable framework for a language model. Even though SPNs have been applied to a variety of vision problems [1, 2], we are the first to use it for language modeling. Our empirical comparisons with six previous language models indicate that our SPN has superior performance.},
  author    = {Wei Chen Cheng and Stanley Kok and Hoai Vu Pham and Hai Leong Chieu and Kian Ming A. Chai},
  issn      = {19909772},
  booktitle = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
  keywords  = {Deep learning,Language models,Probabilistic graphical models,Sum-product networks},
  pages     = {2098-2102},
  title     = {Language modeling with sum-product networks},
  year      = {2014}
}
@inproceedings{Jaini2016,
  author    = {Priyank Jaini and Abdullah Rashwan and Han Zhao and Yue Liu and Ershad Banijamali and Zhitang Chen and Pascal Poupart},
  city      = {Lugano, Switzerland},
  editor    = {Alessandro Antonucci and Giorgio Corani and Cassio Polpo Campos},
  booktitle = {Proceedings of the Eighth International Conference on Probabilistic Graphical Models},
  pages     = {228-239},
  publisher = {PMLR},
  title     = {Online Algorithms for Sum-Product Networks with Continuous Variables},
  year      = {2016}
}
@inproceedings{Gens2013,
  abstract  = {Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their ex-pressiveness. At each step, the algorithm at-tempts to divide the current variables into approximately independent subsets. If suc-cessful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of simi-lar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy.},
  author    = {Robert Gens and Pedro Domingos},
  city      = {Atlanta, GA, USA},
  editor    = {Sanjoy Dasgupta and David McAllester},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  pages     = {873-880},
  publisher = {PMLR},
  title     = {Learning the structure of sum-product networks},
  year      = {2013}
}
@inproceedings{Gens2012,
  abstract  = {Sum-product networks are a new deep architecture that can perform fast, exact inference on high-treewidth models. Only generative methods for training SPNs have been proposed to date. In this paper, we present the ﬁrst discriminative training algorithms for SPNs, combining the high accuracy of the former with the representational power and tractability of the latter. We show that the class of tractable discriminative SPNs is broader than the class of tractable generative ones, and propose an efﬁcient backpropagation-style algorithm for computing the gradient of the conditional log likelihood. Standard gradient descent suffers from the diffusion problem, but networks with many layers can be learned reliably using “hard” gradient descent, where marginal inference is replaced by MPE inference (i.e., inferring the most probable state of the non-evidence variables). The resulting updates have a simple and intuitive form. We test discriminative SPNs on standard image classiﬁcation tasks. We obtain the best results to date on the CIFAR-10 dataset, using fewer features than prior methods with an SPN architecture that learns local image structure discriminatively. We also report the highest published test accuracy on STL-10 even though we only use the labeled portion of the dataset.},
  author    = {Robert Gens and Pedro Domingos},
  city      = {Lake Tahoe, NV, USA},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
  pages     = {3239-3247},
  publisher = {Curran Associates Inc.},
  title     = {Discriminative learning of sum-product networks},
  year      = {2012}
}
@article{Peharz2016,
  abstract = {One of the central themes in Sum-Product networks (SPNs) is the interpretation of sum nodes as marginalized latent variables (LVs). This interpretation yields an increased syntactic or semantic structure, allows the application of the EM algorithm and to efficiently perform MPE inference. In literature, the LV interpretation was justified by explicitly introducing the indicator variables corresponding to the LVs' states. However, as pointed out in this paper, this approach is in conflict with the completeness condition in SPNs and does not fully specify the probabilistic model. We propose a remedy for this problem by modifying the original approach for introducing the LVs, which we call SPN augmentation. We discuss conditional independencies in augmented SPNs, formally establish the probabilistic interpretation of the sum-weights and give an interpretation of augmented SPNs as Bayesian networks. Based on these results, we find a sound derivation of the EM algorithm for SPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature was never proven to be correct. We show that this is indeed a correct algorithm, when applied to selective SPNs, and in particular when applied to augmented SPNs. Our theoretical results are confirmed in experiments on synthetic data and 103 real-world datasets.},
  author   = {Robert Peharz and Robert Gens and Franz Pernkopf and Pedro Domingos},
  issue    = {10},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {MPE inference,Sum-product networks,expectation-maximization,latent variables,mixture models},
  month    = {1},
  pages    = {2030-2044},
  title    = {On the Latent Variable Interpretation in Sum-Product Networks},
  volume   = {39},
  year     = {2016}
}
@inproceedings{Rooshenas2014,
  abstract  = {Copyright © (2014) by the International Machine Learning Society (IMLS) All rights reserved. Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference. SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures. Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables. In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct in-teractions among variables. In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.},
  author    = {Amirmohammad Rooshenas and Daniel Lowd},
  city      = {Beijing, China},
  booktitle = {31st International Conference on Machine Learning, ICML 2014},
  pages     = {I-710-I-718},
  publisher = {JMLR.org},
  title     = {Learning sum-product networks with direct and indirect variable interactions},
  year      = {2014}
}
@article{Darwiche2003,
  abstract = {We present a new approach to inference in Bayesian networks, which is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial. The network polynomial itself is exponential in size, but we show how it can be computed efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size. The proposed framework for inference subsumes one of the most influential methods for inference in Bayesian networks, known as the tree-clustering or jointree method, which provides a deeper understanding of this classical method and lifts its desirable characteristics to a much more general setting. We discuss some theoretical and practical implications of this subsumption.},
  author   = {Adnan Darwiche},
  issue    = {3},
  journal  = {Journal of the ACM},
  keywords = {Bayesian networks,Circuit complexity,Compiling probabilistic models,Probabilistic reasoning},
  month    = {5},
  pages    = {280-305},
  title    = {A differential approach to inference in Bayesian networks},
  volume   = {50},
  year     = {2003}
}
@book{Bondy2008,
  author    = {John Adrian Bondy and Uppaluri Siva Ramachandra Murty},
  edition   = {1},
  pages     = {663},
  publisher = {Springer-Verlag London},
  title     = {Graph Theory},
  year      = {2008}
}
@inproceedings{Mei2017,
  abstract  = {Sum-product networks (SPNs) are a class of probabilistic graphical models that allow tractable marginal inference. However, the maximum a posteriori (MAP) inference in SPNs is NP-hard. We investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we reduce general MAP inference to its special case without evidence and hidden variables; we also show that it is NP-hard to approximate the MAP problem to $2^\{n^\epsilon\}$ for fixed $0 \leq \epsilon < 1$, where $n$ is the input size. For the algorithmic part, we first present an exact MAP solver that runs reasonably fast and could handle SPNs with up to 1k variables and 150k arcs in our experiments. We then present a new approximate MAP solver with a good balance between speed and accuracy, and our comprehensive experiments on real-world datasets show that it has better overall performance than existing approximate solvers.},
  author    = {Jun Mei and Yong Jiang and Kewei Tu},
  booktitle = {AAAI Conference on Artificial Intelligence},
  title     = {Maximum A Posteriori Inference in Sum-Product Networks},
  year      = {2018}
}
@inproceedings{Conaty2017,
  abstract  = {We discuss the computational complexity of approximating maximum a posteriori inference in sum-product networks. We first show NP-hardness in trees of height two by a reduction from maximum independent set; this implies non-approximability within a sublinear factor. We show that this is a tight bound, as we can find an approximation within a linear factor in networks of height two. We then show that, in trees of height three, it is NP-hard to approximate the problem within a factor $2^\{f(n)\}$ for any sublinear function $f$ of the size of the input $n$. Again, this bound is tight, as we prove that the usual max-product algorithm finds (in any network) approximations within factor $2^\{c \cdot n\}$ for some constant $c < 1$. Last, we present a simple algorithm, and show that it provably produces solutions at least as good as, and potentially much better than, the max-product algorithm. We empirically analyze the proposed algorithm against max-product using synthetic and realistic networks.},
  author    = {Diarmaid Conaty and Denis D. Mauá and Cassio P. de Campos},
  editor    = {Gal Elidan and Kristian Kersting},
  booktitle = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence},
  month     = {3},
  pages     = {322–331},
  publisher = {AUAI Press},
  title     = {Approximation Complexity of Maximum A Posteriori Inference in Sum-Product Networks},
  year      = {2017}
}
@inproceedings{Park2002,
  abstract  = {MAP is the problem of finding a most probable instantiation of a set of nvariables in a Bayesian network, given some evidence. MAP appears to be a significantly harder problem than the related problems of computing the probability of evidence Pr, or MPE a special case of MAP. Because of the complexity of MAP, and the lack of viable algorithms to approximate it,MAP computations are generally avoided by practitioners. This paper investigates the complexity of MAP. We show that MAP is complete for NP. We also provide negative complexity results for elimination based algorithms. It turns out that MAP remains hard even when MPE, and Pr are easy. We show that MAP is NPcomplete when the networks are restricted to polytrees, and even then can not be effectively approximated. Because there is no approximation algorithm with guaranteed results, we investigate best effort approximations. We introduce a generic MAP approximation framework. As one instantiation of it, we implement local search coupled with belief propagation BP to approximate MAP. We show how to extract approximate evidence retraction information from belief propagation which allows us to perform efficient local search. This allows MAP approximation even on networks that are too complex to even exactly solve the easier problems of computing Pr or MPE. Experimental results indicate that using BP and local search provides accurate MAP estimates in many cases.},
  author    = {James D. Park},
  city      = {San Francisco, CA, USA},
  booktitle = {Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence},
  month     = {12},
  pages     = {388-396},
  publisher = {Morgan Kaufmann Publishers Inc.},
  title     = {MAP Complexity Results and Approximation Methods},
  year      = {2002}
}
@book{Murphy2012,
  author    = {Kevin Patrick Murphy},
  publisher = {The MIT Press},
  title     = {Machine Learning: A Probabilistic Perspective},
  year      = {2012}
}
@article{Fukunaga1975,
  author  = {K. Fukunaga and L. Hostetler},
  issue   = {1},
  journal = {IEEE Transactions on Information Theory},
  month   = {1},
  pages   = {32-40},
  title   = {The estimation of the gradient of a density function, with applications in pattern recognition},
  volume  = {21},
  year    = {1975}
}
@book{Jaynes2003,
  author    = {Edwin Thompson Jaynes},
  publisher = {Cambridge University Press},
  title     = {Probability Theory: The logic of science},
  year      = {2003}
}
@inproceedings{PTPD2015,
  abstract  = {Sum-product networks (SPNs) are a promising avenue for probabilistic modeling and have been successfully applied to various tasks. However, some theoretic properties about SPNs are not yet well understood. In this paper we fill some gaps in the theoretic foundation of SPNs. First, we show that the weights of any complete and consistent SPN can be transformed into locally normalized weights without changing the SPN distribu-tion. Second, we show that consistent SPNs cannot model distributions significantly (ex-ponentially) more compactly than decompos-able SPNs. As a third contribution, we ex-tend the inference mechanisms known for SPNs with finite states to generalized SPNs with arbitrary input distributions.},
  author    = {Robert Peharz and Sebastian Tschiatschek and Franz Pernkopf and Pedro Domingos},
  city      = {San Diego, CA, USA},
  editor    = {Guy Lebanon and S. V. N. Vishwanathan},
  booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages     = {744-752},
  publisher = {PMLR},
  title     = {On theoretical properties of sum-product networks},
  year      = {2015}
}
@inproceedings{Dennis2012,
  abstract  = {The sum-product network (SPN) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion. Designing an SPN network architecture that is suitable for the task at hand is an open question. We propose an algorithm for learning the SPN architecture from data. The idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another. Nodes in the SPN network are then allocated towards explaining these interactions. Experimental evidence shows that learning the SPN architecture significantly improves its performance compared to using a previously-proposed static architecture.},
  author    = {Aaron Dennis and Dan Ventura},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2033-2041},
  publisher = {Curran Associates, Inc.},
  title     = {Learning the architecture of sum-product networks using clustering on variables},
  year      = {2012}
}
@article{Park2004,
  abstract = {MAP is the problem of finding a most probable instantiation of a set of variables given evidence. MAP has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation (Pr), or the problem of computing the most probable explanation (MPE). This paper investigates the complexity of MAP in Bayesian networks. Specifically, we show that MAP is complete for NP PP and provide further negative complexity results for algorithms based on variable elimination. We also show that MAP remains hard even when MPE and Pr become easy. For example, we show that MAP is NP-complete when the networks are restricted to polytrees, and even then can not be effectively approximated. Given the difficulty of computing MAP exactly, and the difficulty of approximating MAP while providing useful guarantees on the resulting approximation, we investigate best effort approximations. We introduce a generic MAP approximation framework. We provide two instantiations of the framework; one for networks which are amenable to exact inference (Pr), and one for networks for which even exact inference is too hard. This allows MAP approximation on networks that are too complex to even exactly solve the easier problems, Pr and MPE. Experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques, and provide accurate MAP estimates in many cases. © 2004 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
  author   = {James D. Park and Adnan Darwiche},
  issue    = {1},
  journal  = {Journal of Artificial Intelligence Research},
  pages    = {101-133},
  title    = {Complexity results and approximation strategies for MAP explanations},
  volume   = {21},
  year     = {2004}
}
@article{Szekely2007,
  abstract = {Distance correlation is a new measure of dependence between random vectors. Distance covariance and distance correlation are analogous to product-moment covariance and correlation, but unlike the classical definition of correlation, distance correlation is zero only if the random vectors are independent. The empirical distance dependence measures are based on certain Euclidean distances between sample elements rather than sample moments, yet have a compact representation analogous to the classical covariance and correlation. Asymptotic properties and applications in testing independence are discussed. Implementation of the test and Monte Carlo results are also presented. © Institute of Mathematical Statistics, 2007.},
  author   = {Gábor J. Székely and Maria L. Rizzo and Nail K. Bakirov},
  issue    = {6},
  journal  = {Annals of Statistics},
  keywords = {Distance correlation,Distance covariance,Multivariate independence},
  pages    = {2769-2794},
  title    = {Measuring and testing dependence by correlation of distances},
  volume   = {35},
  year     = {2007}
}
@article{Woolf1957,
  abstract = {The theoretical basis of the log likelihood ratio test (the G‐test) is described, and instructions and tables are given for its use as a test of heterogeneity in contingency tables. There is a marked saving in computation time over the customary Karl Pearson test. It is a pleasure to thank Miss Mary Wheeler and Miss Madge Wight for the major part of the computations. Copyright © 1957, Wiley Blackwell. All rights reserved},
  author   = {Barnet Woolf},
  journal  = {Annals of Human Genetics},
  pages    = {397-409},
  title    = {The log likelihoold ratio test (the G-test)},
  volume   = {21},
  year     = {1957}
}
@article{Comaniciu2002,
  abstract = {A general nonparametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure, the mean shift. We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators of location is also established. Algorithms for two low-level vision tasks, discontinuity preserving smoothing and image segmentation, are described as applications. In these algorithms, the only user set parameter is the resolution of the analysis and either gray level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.},
  author   = {Dorin Comaniciu and Peter Meer},
  issue    = {5},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Clustering,Feature space,Image segmentation,Image smoothing,Low-level vision,Mean shift},
  pages    = {603-619},
  title    = {Mean shift: A robust approach toward feature space analysis},
  volume   = {24},
  year     = {2002}
}
@inproceedings{Zhao2016,
  abstract  = {We present a unified approach for learning the parameters of Sum-Product networks (SPNs). We prove that any complete and decomposable SPN is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions. Based on the mixture model perspective, we characterize the objective function when learning SPNs based on the maximum likelihood estimation (MLE) principle and show that the optimization problem can be formulated as a signomial program. We construct two parameter learning algorithms for SPNs by using sequential monomial approximations (SMA) and the concave-convex procedure (CCCP), respectively. The two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation. With the help of the unified framework, we also show that, in the case of SPNs, CCCP leads to the same algorithm as Expectation Maximization (EM) despite the fact that they are different in general.},
  author    = {Han Zhao and Pascal Poupart and Geoff Gordon},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {433-441},
  publisher = {Curran Associates, Inc.},
  title     = {A unified approach for learning the parameters of Sum-Product Networks},
  year      = {2016}
}
@inproceedings{Dennis2015,
  abstract  = {Sum-product networks (SPNs) are rooted, directed acyclic graphs (DAGs) of sum and product nodes with well-defined probabilistic semantics. Moreover, exact inference in the distribution represented by an SPN is guaranteed to take linear time in the size of the DAG. In this paper we introduce an algorithm that learns the structure of an SPN using a greedy search approach. It incorporates methods used in a previous SPN structure-learning algorithm, but, unlike the previous algorithm, is not limited to learning tree-structured SPNs. Several proven ideas from circuit complexity theory along with our experimental results provide evidence for the advantages of SPNs with less-restrictive, nontree structures.},
  author    = {Aaron Dennis and Dan Ventura},
  city      = {Buenos Aires, Argentina},
  booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
  pages     = {932-938},
  publisher = {AAAI Press},
  title     = {Greedy structure search for sum-product networks},
  year      = {2015}
}
@book{Kadane2011,
  author    = {Joseph Born Kadane},
  pages     = {504},
  publisher = {Chapman and Hall},
  title     = {Principles of Uncertainty},
  year      = {2011}
}
@article{Desana2016,
  abstract = {Sum-Product Networks with complex probability distribution at the leaves have been shown to be powerful tractable-inference probabilistic models. However, while learning the internal parameters has been amply studied, learning complex leaf distribution is an open problem with only few results available in special cases. In this paper we derive an efficient method to learn a very large class of leaf distributions with Expectation-Maximization. The EM updates have the form of simple weighted maximum likelihood problems, allowing to use any distribution that can be learned with maximum likelihood, even approximately. The algorithm has cost linear in the model size and converges even if only partial optimizations are performed. We demonstrate this approach with experiments on twenty real-life datasets for density estimation, using tree graphical models as leaves. Our model outperforms state-of-the-art methods for parameter learning despite using SPNs with much fewer parameters.},
  author   = {Mattia Desana and Christoph Schnörr},
  month    = {4},
  title    = {Learning Arbitrary Sum-Product Network Leaves with Expectation-Maximization},
  year     = {2016}
}
@inproceedings{Chan2006,
  abstract  = {In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with the highest probability given the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter changes. Specifically, we ask the question: How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a procedure, which is the first of its kind, that computes this answer for all parameters in the Bayesian network in time O(nexp(w)), where n is the number of network variables and w is its treewidth.},
  author    = {Hei Chan and Adnan Darwiche},
  city      = {Cambridge, MA, USA},
  booktitle = {Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence, UAI 2006},
  pages     = {63-71},
  title     = {On the robustness of most probable explanations},
  year      = {2006}
}
@article{Carreira-Perpinan2000,
  abstract = {Gradient-quadratic and fixed-point iteration algorithms and appropriate values for their control parameters are derived for finding all modes of a Gaussian mixture, a problem with applications in clustering and regression. The significance of the modes found is quantified locally by Hessian-based error bars and globally by the entropy as sparseness measure. © 2000 IEEE.},
  author   = {Miguel Á Carreira-Perpiñán},
  issue    = {11},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Bump finding,Error bars,Gaussian mixtures,Maximization algorithms,Mode finding,Sparseness},
  pages    = {1318-1323},
  title    = {Mode-finding for mixtures of Gaussian distributions},
  volume   = {22},
  year     = {2000}
}
@inbook{Carreira-Perpinan2015,
  abstract  = {A natural way to characterize the cluster structure of a dataset is by finding regions containing a high density of data. This can be done in a nonparametric way with a kernel density estimate, whose modes and hence clusters can be found using mean-shift algorithms. We describe the theory and practice behind clustering based on kernel density estimates and mean-shift algorithms. We discuss the blurring and nonblurring versions of mean-shift; theoretical results about mean-shift algorithms and Gaussian mixtures; relations with scale-space theory, spectral clustering and other algorithms; extensions to tracking, to manifold and graph data, and to manifold denoising; K-modes and Laplacian K-modes algorithms; acceleration strategies for large datasets; and applications to image segmentation, manifold denoising and multivalued regression.},
  author    = {Miguel Á Carreira-Perpiñán},
  journal   = {Handbook of Cluster Analysis},
  pages     = {383-418},
  publisher = {CRC/Chapman and Hall},
  title     = {Clustering methods based on kernel density estimators: Mean-shift algorithms},
  year      = {2015}
}
@article{Carreira-Perpinan2007,
  abstract = {The mean-shift algorithm, based on ideas proposed by Fukunaga and Hostetler [16], is a hill-climbing algorithm on the density defined by a finite mixture or a kernel density estimate. Mean-shift can be used as a nonparametric clustering method and has attracted recent attention in computer vision applications such as image segmentation or tracking. We show that, when the kernel is Gaussian, mean-shift is an expectation-maximization (EM) algorithm and, when the kernel is non-Gaussian, mean-shift is a generalized EM algorithm. This implies that mean-shift converges from almost any starting point and that, in general, its convergence is of linear order. For Gaussian mean-shift, we show: 1) the rate of linear convergence approaches 0 (superlinear convergence) for very narrow or very wide kernels, but is often close to 1 (thus, extremely slow) for intermediate widths and exactly 1 (sublinear convergence) for widths at which modes merge, 2) the iterates approach the mode along the local principal component of the data points from the inside of the convex hull of the data points, and 3) the convergence domains are nonconvex and can be disconnected and show fractal behavior. We suggest ways of accelerating mean-shift based on the EM interpretation. © 2007 IEEE.},
  author   = {Miguel Á Carreira-Perpiñán},
  issue    = {5},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Clustering,EM algorithm,Gaussian mixtures,Kernel density estimators,Mean-shift algorithm},
  pages    = {767-776},
  title    = {Gaussian mean-shift is an EM algorithm},
  volume   = {29},
  year     = {2007}
}
@inproceedings{Carreira-Perpinan2003,
  abstract  = {We consider a problem intimately related to the creation of maxima under Gaussian blurring: the number of modes of a Gaussian mixture in D dimensions. To our knowledge, a general answer to this question is not known. We conjecture that if the components of the mixture have the same covariance matrix (or the same covariance matrix up to a scaling factor), then the number of modes cannot exceed the number of components. We demonstrate that the number of modes can exceed the number of components when the components are allowed to have arbitrary and different covariance matrices. We will review related results from scale-space theory, statistics and machine learning, including a proof of the conjecture in 1D. We present a convergent, EM-like algorithm for mode finding and compare results of searching for all modes starting from the centers of the mixture components with a brute-force search. We also discuss applications to data reconstruction and clustering. © Springer-Verlag Berlin Heidelberg 2003.},
  author    = {Miguel Á Carreira-Perpiñán and Christopher K.I. Williams},
  city      = {Berlin, Heidelberg},
  editor    = {Lewis D. Griffin and Martin Lillholm},
  booktitle = {Scale Space Methods in Computer Vision},
  pages     = {625-640},
  publisher = {Springer Berlin Heidelberg},
  title     = {On the number of modes of a Gaussian mixture},
  volume    = {2695},
  year      = {2003}
}
@article{Carreira-Perpinan2003a,
  author  = {Miguel Á Carreira-Perpiñán and Christopher K.I. Williams},
  issue   = {2},
  journal = {Institute for Adaptive and Neural Computation},
  title   = {An isotropic Gaussian mixture can have more modes than components},
  volume  = {4},
  year    = {2003}
}
@article{Amendola2019,
  abstract = {Gaussian mixture models are widely used in Statistics. A fundamental aspect of these distributions is the study of the local maxima of the density, or modes. In particular, it is not known how many modes a mixture of $k$ Gaussians in $d$ dimensions can have. We give a brief account of this problem's history. Then, we give improved lower bounds and the first upper bound on the maximum number of modes, provided it is finite.},
  author   = {Carlos Améndola and Alexander Engström and Christian Haase},
  journal  = {Information and Inference: A Journal of the IMA},
  month    = {2},
  title    = {Maximum Number of Modes of Gaussian Mixtures},
  year     = {2019}
}
@book{Russell2010,
  abstract = {The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed},
  author   = {Stuart Russell and Peter Norvig},
  edition  = {3},
  journal  = {Pearson},
  title    = {Artificial Intelligence A Modern Approach},
  year     = {2010}
}
@article{Clausen1999,
  abstract = {A large number of real-world planning problems called combinatorial optimization problems share the following properties: They are optimiza- tion problems, are easy to state, and have a finite but usually very large number of feasible solutions. While some of these as e.g. the Shortest Path problem and the Minimum Spanning Tree problem have polynomial algo- ritms, the majority of the problems in addition share the property that no polynomial method for their solution is known. Examples here are vehicle routing, crew scheduling, and production planning. All of these problems are NP-hard. Branch and Bound (B&B) is by far the most widely used tool for solv- ing large scale NP-hard combinatorial optimization problems. B&B is, however, an algorithm paradigm, which has to be filled out for each spe- cific problem type, and numerous choices for each of the components ex- ist. Even then, principles for the design of efficient B&B algorithms have emerged over the years. In this paper I review the main principles of B&B and illustrate the method and the different design issues through three examples: the Sym- metric Travelling Salesman Problem, the Graph Partitioning problem, and the Quadratic Assignment problem.},
  author   = {Jens Clausen},
  journal  = {Department of Computer Science, University of Copenhagen},
  pages    = {1-30},
  title    = {Branch and bound algorithms-principles and examples},
  year     = {1999}
}
@article{Little1963,
  author  = {John D. C. Little and Katta G. Murty and Dura W. Sweeney and Caroline Karel},
  issue   = {6},
  journal = {Operations Research},
  month   = {12},
  pages   = {972-989},
  title   = {An Algorithm for the Traveling Salesman Problem},
  volume  = {11},
  year    = {1963}
}
@inproceedings{Molina2018,
  abstract  = {While all kinds of mixed data'from personal data, over panel and scientific data, to public and commercial data'are collected and stored, building probabilistic graphical models for these hybrid domains becomes more difficult. Users spend significant amounts of time in identifying the parametric form of the random variables (Gaussian, Poisson, Logit, etc.) involved and learning the mixed models. To make this difficult task easier, we propose the first trainable probabilistic deep architecture for hybrid domains that features tractable queries. It is based on Sum-Product Networks (SPNs) with piecewise polynomial leaf distributions together with novel nonparametric decomposition and conditioning steps using the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. This relieves the user from deciding a-priori the parametric form of the random variables but is still expressive enough to effectively approximate any distribution and permits efficient learning and inference. Our experiments show that the architecture, called Mixed SPNs, can indeed capture complex distributions across a wide range of hybrid domains.},
  author    = {Alejandro Molina and Sriraam Natarajan and Antonio Vergari and Floriana Esposito and Nicola Di Mauro and Kristian Kersting},
  booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
  title     = {Mixed sum-product networks: A deep architecture for hybrid domains},
  year      = {2018}
}
@article{Li2007,
  author  = {Jia Li and Surajit Ray and Bruce G Lindsay},
  issue   = {59},
  journal = {Journal of Machine Learning Research},
  pages   = {1687-1723},
  title   = {A Nonparametric Statistical Approach to Clustering via Mode Identification},
  volume  = {8},
  year    = {2007}
}
@inproceedings{Maua2020,
  author    = {Denis Deratani Mauá and Heitor Reis Ribeiro and Gustavo Perez Katague and Alessandro Antonucci},
  editor    = {Manfred Jaeger and Thomas Dyhre Nielsen},
  booktitle = {Proceedings of the Tenth International Conference on Probabilistic Graphical Models},
  pages     = {293-304},
  publisher = {PMLR},
  title     = {Two Reformulation Approaches to Maximum-A-Posteriori Inference in Sum-Product Networks},
  volume    = {138},
  year      = {2020}
}
@misc{Molina2019SPFlow,
  author = {Alejandro Molina and Antonio Vergari and Karl Stelzner and Robert Peharz and Pranav Subramani and Nicola Di Mauro and Pascal Poupart and Kristian Kersting},
  title  = {SPFlow: An Easy and Extensible Library for Deep Probabilistic Learning using Sum-Product Networks},
  year   = {2019}
}
@article{Njah2021,
  abstract = {Clustering high-dimensional data under the curse of dimensionality is an arduous task in many applications domains. The wide dimension yields the complexity-related challenges and the limited number of records leads to the overfitting trap. We propose to tackle this problematic using the graphical and probabilistic power of the Bayesian network. Our contribution is a new loose hierarchical Bayesian network model that encloses latent variables. These hidden variables are introduced for ensuring a multi-view clustering of the records. We propose a new framework for learning our proposed Bayesian network model. It starts by extracting the cliques of highly dependent features and it proceeds to learn representative latent variable for each features’ clique. The experimental results of our comparative analysis prove the efficiency of our model in tackling the distance concentration challenge. They also show the effectiveness of our model learning framework in skipping the overfitting trap, on benchmark high-dimensional datasets.},
  author   = {Hasna Njah and Salma Jamoussi and Walid Mahdi},
  doi      = {10.1007/s10472-021-09749-z},
  issn     = {1573-7470},
  issue    = {10},
  journal  = {Annals of Mathematics and Artificial Intelligence},
  pages    = {1013-1033},
  title    = {Breaking the curse of dimensionality: hierarchical Bayesian network model for multi-view clustering},
  volume   = {89},
  year     = {2021}
}
@inproceedings{Llerena2017,
  author    = {Julissa Villanueva Llerena and Denis Deratani Maua},
  doi       = {10.1109/BRACIS.2017.34},
  isbn      = {978-1-5386-2407-4},
  booktitle = {2017 Brazilian Conference on Intelligent Systems (BRACIS)},
  month     = {10},
  pages     = {25-30},
  publisher = {IEEE},
  title     = {On Using Sum-Product Networks for Multi-label Classification},
  year      = {2017}
}
@inproceedings{Madeira2022,
  author    = {Tiago Madeira and Denis Mauá},
  city      = {Porto Alegre, RS, Brasil},
  doi       = {10.5753/eniac.2022.227582},
  issn      = {2763-9061},
  booktitle = {Anais do XIX Encontro Nacional de Inteligência Artificial e Computacional},
  pages     = {497-508},
  publisher = {SBC},
  title     = {Tractable Mode-Finding in Sum-Product Networks with Gaussian Leaves},
  year      = {2022}
}
@inproceedings{Marzagao2021,
  abstract  = {Multi-agent consensus problems can often be seen as a sequence of autonomous and independent local choices between a finite set of decision options, with each local choice undertaken simultaneously, and with a shared goal of achieving a global consensus state. Being able to estimate probabilities for the different outcomes and to predict how long it takes for a consensus to be formed, if ever, are core issues for such protocols. Little attention has been given to protocols in which agents can remember past or outdated states. In this paper, we propose a framework to study what we call memory consensus protocol. We show that the employment of memory allows such processes to always converge, as well as, in some scenarios, such as cycles, converge faster. We provide a theoretical analysis of the probability of each option eventually winning such processes based on the initial opinions expressed by agents. Further, we perform experiments to investigate network topologies in which agents benefit from memory on the expected time needed for consensus.},
  author    = {D.K. Marzagão and L.B. Bonatto and T. Madeira and M.M. Gauy and P. McBurney},
  isbn      = {9781713835974},
  booktitle = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
  title     = {The Influence of Memory in Multi-Agent Consensus},
  volume    = {13A},
  year      = {2021}
}
@article{Ray2012,
  author  = {Surajit Ray and Dan Ren},
  doi     = {10.1016/j.jmva.2012.02.006},
  issn    = {0047259X},
  journal = {Journal of Multivariate Analysis},
  month   = {7},
  pages   = {41-52},
  title   = {On the upper bound of the number of modes of a multivariate normal mixture},
  volume  = {108},
  year    = {2012}
}
@article{Dempster1977,
  author  = {A. P. Dempster and N. M. Laird and D. B. Rubin},
  doi     = {10.1111/j.2517-6161.1977.tb01600.x},
  issn    = {00359246},
  issue   = {1},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  month   = {9},
  pages   = {1-22},
  title   = {Maximum Likelihood from Incomplete Data Via the EM Algorithm},
  volume  = {39},
  year    = {1977}
}
@article{Edelsbrunner2013,
  author  = {Herbert Edelsbrunner and Brittany Terese Fasy and Günter Rote},
  doi     = {10.1007/s00454-013-9517-x},
  issn    = {0179-5376},
  issue   = {4},
  journal = {Discrete and Computational Geometry},
  month   = {6},
  pages   = {797-822},
  title   = {Add Isotropic Gaussian Kernels at Own Risk: More and More Resilient Modes in Higher Dimensions},
  volume  = {49},
  year    = {2013}
}
@article{Scrucca2021,
  author  = {Luca Scrucca},
  doi     = {10.1002/sam.11527},
  issn    = {1932-1864},
  issue   = {4},
  journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
  month   = {8},
  pages   = {305-314},
  title   = {A fast and efficient Modal EM algorithm for Gaussian mixtures},
  volume  = {14},
  year    = {2021}
}
@article{Yao2013,
  author  = {Weixin Yao},
  doi     = {10.1016/j.spl.2012.10.017},
  issn    = {01677152},
  issue   = {2},
  journal = {Statistics and Probability Letters},
  month   = {2},
  pages   = {519-526},
  title   = {A note on EM algorithm for mixture models},
  volume  = {83},
  year    = {2013}
}
@article{Wu1983,
  author  = {C. F. Jeff Wu},
  doi     = {10.1214/aos/1176346060},
  issn    = {0090-5364},
  issue   = {1},
  journal = {The Annals of Statistics},
  month   = {3},
  title   = {On the Convergence Properties of the EM Algorithm},
  volume  = {11},
  year    = {1983}
}
@inproceedings{Shen2005,
  author    = {Chunhua Shen and M.J. Brooks and A. van den Hengel},
  doi       = {10.1109/ICCV.2005.94},
  isbn      = {0-7695-2334-X},
  booktitle = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
  pages     = {1516-1523 Vol. 2},
  publisher = {IEEE},
  title     = {Fast global kernel density mode seeking with application to localization and tracking},
  year      = {2005}
}
@article{Pulkkinen2014,
  author = {Seppo Pulkkinen},
  month  = {4},
  title  = {Efficient Optimization Algorithms for Nonlinear Data Analysis},
  year   = {2014}
}
@article{Pulkkinen2013,
  author  = {Seppo Pulkkinen and Marko Mikael Mäkelä and Napsu Karmitsa},
  doi     = {10.1007/s10898-011-9833-8},
  issn    = {0925-5001},
  issue   = {2},
  journal = {Journal of Global Optimization},
  month   = {6},
  pages   = {459-487},
  title   = {A continuation approach to mode-finding of multivariate Gaussian mixtures and kernel density estimates},
  volume  = {56},
  year    = {2013}
}
@book{Titterington1985,
  author    = {D M Titterington and A F M Smith and U E Makov},
  isbn      = {9780471907633},
  publisher = {Wiley},
  title     = {Statistical Analysis of Finite Mixture Distributions},
  year      = {1985}
}
@inproceedings{Geh2021,
  author    = {Renato Geh and Denis Mauá},
  booktitle = {The 4th Workshop on Tractable Probabilistic Modeling},
  title     = {Fast And Accurate Learning of Probabilistic Circuits by Random Projections},
  year      = {2021}
}
@inproceedings{Peharz2020,
  abstract  = {Sum-product networks (SPNs) are expressive probabilistic models with a rich set of exact and efficient inference routines. However, in order to guarantee exact inference, they require specific structural constraints, which complicate learning SPNs from data. Thereby, most SPN structure learners proposed so far are tedious to tune, do not scale easily, and are not easily integrated with deep learning frameworks. In this paper, we follow a simple “deep learning” approach, by generating unspecialized random structures, scalable to millions of parameters, and subsequently applying GPU-based optimization. Somewhat surprisingly, our models often perform on par with state-of-the-art SPN structure learners and deep neural networks on a diverse range of generative and discriminative scenarios. At the same time, our models yield well-calibrated uncertainties, and stand out among most deep generative and discriminative models in being robust to missing features and being able to detect anomalies.},
  author    = {Robert Peharz and Antonio Vergari and Karl Stelzner and Alejandro Molina and Xiaoting Shao and Martin Trapp and Kristian Kersting and Zoubin Ghahramani},
  editor    = {Ryan P Adams and Vibhav Gogate},
  booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  month     = {4},
  pages     = {334-344},
  publisher = {PMLR},
  title     = {Random Sum-Product Networks: A Simple and Effective Approach to Probabilistic Deep Learning},
  volume    = {115},
  year      = {2020}
}
@article{Chacon2019,
  author  = {José E. Chacón},
  doi     = {10.1007/s11634-018-0308-3},
  issn    = {1862-5347},
  issue   = {2},
  journal = {Advances in Data Analysis and Classification},
  month   = {6},
  pages   = {379-404},
  title   = {Mixture model modal clustering},
  volume  = {13},
  year    = {2019}
}
@article{Cheng1995,
  author  = {Yizong Cheng},
  doi     = {10.1109/34.400568},
  issn    = {01628828},
  issue   = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages   = {790-799},
  title   = {Mean Shift, mode seeking, and clustering},
  volume  = {17},
  year    = {1995}
}
@article{Carlsson2013,
  author  = {Gunnar Carlsson and Facundo Mémoli},
  doi     = {10.1007/s10208-012-9141-9},
  issn    = {1615-3375},
  issue   = {2},
  journal = {Foundations of Computational Mathematics},
  month   = {4},
  pages   = {221-252},
  title   = {Classifying Clustering Schemes},
  volume  = {13},
  year    = {2013}
}
@article{Vergari2018,
  abstract = {<p>Sum-Product Networks (SPNs) are a deep probabilistic architecture that up to now has been successfully employed for tractable inference. Here, we extend their scope towards unsupervised representation learning: we encode samples into continuous and categorical embeddings and show that they can also be decoded back into the original input space by leveraging MPE inference. We characterize when this Sum-Product Autoencoding (SPAE) leads to equivalent reconstructions and extend it towards dealing with missing embedding information. Our experimental results on several multi-label classification problems demonstrate that SPAE is competitive with state-of-the-art autoencoder architectures, even if the SPNs were never trained to reconstruct their inputs.</p>},
  author   = {Antonio Vergari and Robert Peharz and Nicola Di Mauro and Alejandro Molina and Kristian Kersting and Floriana Esposito},
  doi      = {10.1609/aaai.v32i1.11734},
  issn     = {2374-3468},
  issue    = {1},
  journal  = {Proceedings of the AAAI Conference on Artificial Intelligence},
  month    = {4},
  title    = {Sum-Product Autoencoding: Encoding and Decoding Representations Using Sum-Product Networks},
  volume   = {32},
  year     = {2018}
}
@inproceedings{Lopes-Paz2013,
  abstract  = {We introduce the Randomized Dependence Coefficient (RDC), a measure of nonlinear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.},
  author    = {David Lopez-Paz and Philipp Hennig and Bernhard Schölkopf},
  city      = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
  pages     = {1-9},
  publisher = {Curran Associates Inc.},
  title     = {The Randomized Dependence Coefficient},
  year      = {2013}
}
@inproceedings{MacQueen1967,
  author    = {J B MacQueen},
  editor    = {L M Le Cam and J Neyman},
  booktitle = {Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability},
  keywords  = {kmeans clustering},
  pages     = {281-297},
  publisher = {University of California Press},
  title     = {Some Methods for Classification and Analysis of MultiVariate Observations},
  volume    = {1},
  year      = {1967}
}
@inproceedings{Madeira2023,
  author    = {Tiago Madeira and Denis Mauá},
  booktitle = {The 6th Workshop on Tractable Probabilistic Modeling},
  title     = {On Modal Clustering with Gaussian Sum-Product Networks},
  year      = {2023}
}
